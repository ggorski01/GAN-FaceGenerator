{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/python3\n\nimport tensorflow as tf\nimport numpy as np\nimport os,time,math,sys,random\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Reducing GPU usage:\n\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsess = tf.compat.v1.Session(config = config)\ntf.compat.v1.keras.backend.set_session(sess)   \n\n# Using TPU\n\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# - - - - - - - - - - - - - - - - - - - - - - \n\n# Defining constants:\nIMG_SHAPE = (32, 32, 3)\nNOISE_SHAPE = (100,)\n\nNOISE = tf.random.normal([1, NOISE_SHAPE[0]])\n\nTRAIN_SIZE = 20000\nLR = 0.0002\nBR = 0.5\nOPTIMIZER = tf.compat.v1.keras.optimizers.Adam(LR,BR)\n\nBATCH_SIZE = 32\nHALF_BATCH = int(BATCH_SIZE/2)\nN_EPOCHS = 1000000\n\nGENERATED_IMAGE_PATH = \"./result_GAN/fake_faces/\"\nGENERATED_PLOTS_PATH = \"./result_GAN/plots/\"\nGENERATED_MODEL_PATH =  \"./result_GAN/models/\"\nRESULTS_PATH=\"./result_GAN/\"\n# - - - - - - - - - - - - - - - - - - - - - - ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# - - - - - - - - - - - - - - - - - - - - - - \n# Perform validity check on real versus fake images:\ndef build_discriminator(img_shape, noutput = 1):\n    input_img = tf.keras.layers.Input(shape=img_shape)\n    \n    d = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv1')(input_img)\n    d = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='block1_conv2')(d)\n    d = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(d)\n    \n    d = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block2_conv1')(d)\n    d = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block2_conv2')(d)\n    d = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(d)\n    \n    d = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block4_conv1')(d)\n    d = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block4_conv2')(d)\n    d = tf.keras.layers.MaxPooling2D((2, 2), strides=(1, 1), name='block4_pool')(d)\n    \n    d         = tf.keras.layers.Flatten()(d)\n    d         = tf.keras.layers.Dense(1024,      activation=\"relu\")(d)\n    out       = tf.keras.layers.Dense(noutput,   activation='sigmoid')(d)\n    model     = tf.keras.models.Model(input_img, out)\n    model.summary()\n    return model\n# - - - - - - - - - - - - - - - - - - - - - - ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# - - - - - - - - - - - - - - - - - - - - - - \n# Generate my fake images:\ndef build_generator(img_shape, noise_shape = NOISE_SHAPE):\n    print(img_shape)\n    print(\"hue\")\n    input_noise = tf.keras.layers.Input(shape = noise_shape) \n    g = tf.keras.layers.Dense(1024, activation = \"relu\")(input_noise) \n    g = tf.keras.layers.Dense(1024, activation = \"relu\")(input_noise) \n    g = tf.keras.layers.Dense(128*8*8, activation = \"relu\")(g)\n    g = tf.keras.layers.Reshape((8,8,128))(g)\n    \n    g = tf.keras.layers.Conv2DTranspose(128, kernel_size = (2,2) ,  strides = (2,2) , use_bias = False)(g)\n    g = tf.keras.layers.Conv2D( 64  , ( 1 , 1 ) , activation = 'relu' , padding = 'same', name= \"block_4\")(g) ## 16,16\n\n    g = tf.keras.layers.Conv2DTranspose(32, kernel_size = (2,2) ,  strides = (2,2) , use_bias = False)(g)\n    g = tf.keras.layers.Conv2D( 64  , ( 1 , 1 ) , activation = 'relu' , padding = 'same', name = \"block_5\")(g) ## 32,32\n    \n    \n    if img_shape[0] ==  64:\n        g = layers.Conv2DTranspose(32, kernel_size=(2,2) ,  strides=(2,2) , use_bias=False)(g)\n        g = layers.Conv2D( 64  , ( 1 , 1 ) , activation='relu' , padding='same', name=\"block_6\")(g) ## 64,64\n    \n    img = tf.keras.layers.Conv2D( 3 , ( 1 , 1 ) , activation = 'sigmoid' , padding = 'same', name = \"final_block\")(g) ## 32, 32\n    model = tf.keras.models.Model(input_noise, img)\n    model.summary() \n    return model\n# - - - - - - - - - - - - - - - - - - - - - - \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/python3\n# - - - - - - - - - - - - - - - - - - - - - - \n# Load my train image files into a numpy array:\ndef load_dataset():\n    # Loading dataset\n    dataset = []\n\n    # Get female faces:\n    for dirname, _, filenames in os.walk('../input/celebahq/celeba_hq/train/female'):\n        for i, filename, in enumerate(filenames):\n            #print(os.path.join(dirname, filename))\n            img = tf.compat.v1.keras.preprocessing.image.load_img(os.path.join(dirname, filename), target_size = IMG_SHAPE[:2])\n            img = tf.compat.v1.keras.preprocessing.image.img_to_array(img)/255.0\n            dataset.append(img)\n            if i  == TRAIN_SIZE:\n                break\n            \n    # Get male faces:\n    for dirname, _, filenames in os.walk('../input/celebahq/celeba_hq/train/male'):\n        for i, filename, in enumerate(filenames):\n            #print(os.path.join(dirname, filename))\n            img = tf.compat.v1.keras.preprocessing.image.load_img(os.path.join(dirname, filename), target_size = IMG_SHAPE[:2])\n            img = tf.compat.v1.keras.preprocessing.image.img_to_array(img)/255.0\n            dataset.append(img)\n            if i  == TRAIN_SIZE:\n                break\n    random.shuffle(dataset)\n    return np.array(dataset)\n\n#print(\"Train.shape = {}\".format(load_dataset().shape))\n# - - - - - - - - - - - - - - - - - - - - - -\n\n# - - - - - - - - - - - - - - - - - - - - - -\n# Generate plot of my images:\n\ndef plot_images(sample):\n    fig = plt.figure(figsize = (30, 10))\n    for i in range(1,7):\n        ax = fig.add_subplot(1, 7,i)\n        ax.imshow(sample[i])\n    return plt.show()\n# - - - - - - - - - - - - - - - - - - - - - -\n\ndef get_noise(nsample=1, nlatent_dim=100):\n    noise = np.random.normal(0, 1, (nsample,nlatent_dim))\n    return(noise)\n\n# - - - - - - - - - - - - - - - - - - - - - -\n# Save network models:\ndef save_models(generator, discriminator, epoch):\n    # If no directory, then create:\n    if not os.path.exists(GENERATED_MODEL_PATH):\n        os.mkdir(GENERATED_MODEL_PATH)\n    # Saving models:\n    generator.save(GENERATED_MODEL_PATH+'dcgan_generator_'+str(epoch) + '.h5')\n    discriminator.save(GENERATED_MODEL_PATH+'dcgan_discriminator_'+str(epoch) + '.h5')\n    print(\"\\n MODELS: GENERATOR + DISCRIMINATOR SAVED!\\n\")\n# - - - - - - - - - - - - - - - - - - - - - -\n\n# - - - - - - - - - - - - - - - - - - - - - -\n# Output network training progress:\ndef show_progress(epoch, d_loss, g_loss):\n    print (\n            \"Epoch {:05.0f} [D loss: {:4.3f}, acc.: {:05.1f}%] [G loss: {:4.3f}]\"\n            .format(epoch, d_loss[0], 100*d_loss[1], g_loss)\n            )\n# - - - - - - - - - - - - - - - - - - - - - -\n\ndef plot_generated_images(generator,noise,path_save=None,titleadd=\"\"):\n    if not os.path.exists(GENERATED_PLOTS_PATH):\n        os.mkdir(GENERATED_PLOTS_PATH)\n    imgs = generator.predict(noise)\n    fig = plt.figure(figsize=(40,10))\n    for i, img in enumerate(imgs):\n        ax = fig.add_subplot(1,4,i+1)\n        ax.imshow(img)\n    fig.suptitle(\"Generated images \"+titleadd,fontsize=30)\n    \n    if path_save is not None:\n        plt.savefig(path_save,\n                    bbox_inches='tight',\n                    pad_inches=0)\n        plt.close()\n    else:\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/python3\n\n# https://fairyonice.github.io/My-first-GAN-using-CelebA-data.html\n# https://www.tensorflow.org/tutorials/generative/dcgan\n# https://github.com/vwrs/dcgan-mnist\n# - - - - - - - - - - - - - - - - - - - - - -\n# My Deep Convolution Generative Adversarial Network:\n\n# - - - - - - - - - - - - - - - - - - - - - -\n# DCGAN's model objects:\ndef dcGan_models():\n    #with tpu_strategy.scope():\n    generator = build_generator(IMG_SHAPE, noise_shape = NOISE_SHAPE)\n    generator.compile(\n                loss = 'binary_crossentropy',\n                optimizer = OPTIMIZER)\n\n    discriminator  = build_discriminator(IMG_SHAPE)\n    discriminator.compile(\n                loss = 'binary_crossentropy',\n                optimizer = OPTIMIZER,\n                metrics   = ['accuracy'])\n\n    noise_inputLayer = tf.keras.layers.Input(shape = NOISE_SHAPE)\n    generated_image = generator(noise_inputLayer)\n    discriminator.trainable = False\n    valid = discriminator(generated_image)\n\n    combined = tf.keras.models.Model(noise_inputLayer, valid)\n    combined.compile(\n                loss = 'binary_crossentropy',\n                optimizer = OPTIMIZER)\n    #combined.summary()\n    return (combined, discriminator, generator)\n# - - - - - - - - - - - - - - - - - - - - - -\n\n# - - - - - - - - - - - - - - - - - - - - - -\n# DCGAN network train routine:\ndef train(models, X_train, noise_plot, RESULTS_PATH=\"/result/\", epochs=10000, batch_size=128):\n    combined, discriminator, generator = models\n    nlatent_dim = noise_plot.shape[1]\n    half_batch  = int(batch_size / 2)\n    history = []\n    for epoch in range(epochs):\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        # Select a random half batch of images\n        idx = np.random.randint(0, X_train.shape[0], half_batch)\n        imgs = X_train[idx]\n        noise = get_noise(half_batch, nlatent_dim)\n\n        # Generate a half batch of new images\n        gen_imgs = generator.predict(noise)\n\n\n        # Train the discriminator q: better to mix them together?\n        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n\n        # ---------------------\n        #  Train Generator\n        # ---------------------\n\n        noise = get_noise(batch_size, nlatent_dim)\n\n        # The generator wants the discriminator to label the generated samples\n        # as valid (ones)\n        valid_y = (np.array([1] * batch_size)).reshape(batch_size,1)\n\n        # Train the generator\n        g_loss = combined.train_on_batch(noise, valid_y)\n\n        history.append({\"D\":d_loss[0],\"G\":g_loss})\n\n        # Process progress at each epoch:\n        if epoch % 100 == 0:\n            show_progress(epoch, d_loss, g_loss)\n  \n        if epoch % int(epochs/100) == 0:\n            plot_generated_images(generator,noise_plot,\n                                    path_save=GENERATED_PLOTS_PATH+\"plot_{:05.0f}.png\".format(epoch),\n                                    titleadd=\"Epoch {}\".format(epoch))\n        if epoch % 1000 == 0:\n            plot_generated_images(generator,noise_plot,\n                                    titleadd=\"Epoch {}\".format(epoch))\n            save_models(generator, discriminator, str(epoch))\n                                              \n    #Save final model:\n    save_models(generator, discriminator, \" FINAL \")\n    \n    image = combine_images(generator.predict(noise_plot))\n    image = image*127.5 + 127.5\n    Image.fromarray(image.astype(np.uint8))\\\n            .save(GENERATED_IMAGE_PATH+\"%03.png\" % (epoch))\n    \n    return(history)\n# - - - - - - - - - - - - - - - - - - - - - -\n\ndef combine_images(generated_images):\n    total,width,height = generated_images.shape[:-1]\n    cols = int(math.sqrt(total))\n    rows = math.ceil(float(total)/cols)\n    combined_image = np.zeros((height*rows, width*cols),\n                              dtype=generated_images.dtype)\n\n    for index, image in enumerate(generated_images):\n        i = int(index/cols)\n        j = index % cols\n        combined_image[width*i:width*(i+1), height*j:height*(j+1)] = image[:, :, 0]\n    return combined_image\n\ndef main():\n    print(\"Start\")\n    X_train = load_dataset()\n    noise = get_noise(4, NOISE_SHAPE[0])\n\n    try:\n        os.mkdir(RESULTS_PATH)\n        os.mkdir(GENERATED_PLOTS_PATH)\n        os.mkdir(GENERATED_IMAGE_PATH)\n    except:\n        pass\n\n    start_time = time.time()\n    _models = dcGan_models()\n    history = train(_models, X_train, noise, RESULTS_PATH=RESULTS_PATH,epochs=20000, batch_size=128*8)\n    end_time = time.time()\n    print(\"-\"*10)\n    print(\"Time took: {:4.2f} min\".format((end_time - start_time)/60))\n    \nif __name__ == \"__main__\":\n    main()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}